<!DOCTYPE html>
<html lang="ja">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@pecorarista">
  <meta name="twitter:title" content="Calculating Backpropagation Using Fréchet Derivative — pecorarista.com">
  <meta name="twitter:description" content="">
  <meta name="twitter:image" content="https://pecorarista.com/static/images/avatar.png">
  <link rel="icon" href="/favicon.ico">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP&family=Noto+Serif+JP&display=swap">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto&display=swap">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Amiri&display=swap">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Judson&display=swap">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&display=swap">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootswatch@4.5.2/dist/cosmo/bootstrap.min.css" integrity="sha384-5QFXyVb+lrCzdN228VS3HmzpiE7ZVwLQtkt+0d9W43LQMzz4HBnnqvVxKg6O+04d" crossorigin="anonymous">
  <link rel="stylesheet" href="/static/css/prism-lucario.css" />
  <link rel="stylesheet" href="/static/css/main.css" />

  <title>Calculating Backpropagation Using Fréchet Derivative</title>
</head>

<body>
  <div class="wrapper">
    <header>
      <nav class="navbar navbar-expand-lg navbar-dark">
        <div class="container">
          <div class="navbar-header pull-left"><a class="navbar-brand" href="/">pecorarista.com</a></div>
          <div class="navbar-header pull-right">
            <ul class="navbar-nav">
              <li class="nav-item"><a class="nav-link active" href="/resources">Resources</a></li>
            </ul>
          </div>
        </div>
      </nav>

    </header>
    <main>

      <article>
        <div class="container">
          <h1>Calculating Backpropagation Using Fréchet Derivative</h1>
          <div class="info text-muted">
            Posted: 2022-10-09

          </div>

          <section>
            <p>Consider a multilayer perceptron consisting of \(L\) layers, whose \(\ell\)-th layer has \(n(\ell)\) nodes.
              The loss function is the squared error
              \begin{align*}
              E(z) =
              \frac{1}{2}
              {
              \left\|
              z - y
              \right\|
              }^2,
              \end{align*}
              and the activation function for the \(\ell\)-th layer is
              \(\sigma^{(\ell)}\colon \mathbb{R}^{n(\ell)} \to \mathbb{R}^{n(\ell)}\)
              and
              \(\sigma^{(\ell)}_i\colon \mathbb{R} \to \mathbb{R}\)
              The activation function is expressed as
              \begin{gather*}
              \sigma^{(\ell)}(x) =
              \begin{pmatrix}
              \sigma_1^{(\ell)}(x_1) \\
              \vdots \\
              \sigma_{n(\ell)}^{(\ell)}\left(x_{n(\ell)}\right)
              \end{pmatrix},\\[5pt]
              \end{gather*}
              where each \(\sigma^{(\ell)}_i\colon \mathbb{R} \to \mathbb{R}\) is differentiable.
              Let \(u^{(\ell)} \in \mathbb{R}^{n(\ell)}\) be the input to the \(\ell\)-th layer
              and \(z^{(\ell)} \in \mathbb{R}^{n(\ell)}\) be the output from the \(\ell\)-th layer:
              \begin{align*}
              z^{(\ell)} = \sigma^{(\ell)} \left( u^{(\ell)} \right).
              \end{align*}
              The weight of the \(\ell\)-th layer is
              \(W^{(\ell)} \in \mathbb{R}^{n(\ell) \times n(\ell - 1)}\), which is expressed as
              \begin{align*}
              W^{(\ell)} =
              \begin{pmatrix}
              w_{1,\,1}^{(\ell)} &amp; \cdots &amp; w_{1,\,n(\ell - 1)}^{(\ell)} \\
              \vdots &amp; \ddots &amp; \vdots \\
              w_{n(\ell),\,1}^{(\ell)} &amp; \cdots &amp; w_{n(\ell),\,n(\ell - 1)}
              \end{pmatrix},
              \end{align*}
              so this means that
              \begin{align*}
              u^{(\ell)} = W^{(\ell)} z^{(\ell - 1)} = W^{(\ell)} \sigma^{(\ell - 1)} \left( u^{(\ell - 1)} \right),\quad
              \end{align*}
              I use the symbol \(W\) for the linear map \(z^{(\ell - 1)} \mapsto W^{(\ell) } z^{(\ell - 1)}\) as well.
            <figure class="centered">
              <figcaption><strong>Figure 1:</strong> Architecture of the neural network.</figcaption>
              <img src="/static/images/nn.svg" alt="Architecture of the neural network" style="background-color: transparent; min-width: 700px; margin-top: 2ex;" />
            </figure>
            </p>
            <p>
              The goal of this post is to derive the derivative of \(E\) with respect to \(W^{(L)}\)
              and \(W^{(L - 1)}\) because other cases are easily derived in similar ways.
              First, assume each variable is one-dimensional to aid intuitive understanding.
              Then,
              \begin{gather}
              \frac{dE}{dW^{(L)}} =
              \frac{dE}{dz^{(L)}}
              \frac{dz^{(L)}}{du^{(L)}}
              \frac{du^{(L)}}{dW^{(L)}},
              \label{one-dim-chain} \\
              \end{gather}
              \begin{gather}
              \frac{dE}{dW^{(L - 1)}} =
              \frac{dE}{dz^{(L)}}
              \frac{dz^{(L)}}{du^{(L)}}
              \frac{du^{(L)}}{du^{(L - 1)}}
              \frac{du^{(L - 1)}}{dW^{(L - 1)}}.
              \label{one-dim-chain2}
              \end{gather}
              In general cases where dimensions are greater than one, the following analogous formula holds
              \begin{align}
              D_{W^{(L)}} E\left(W^{(L)}\right)(H) =
              \left(D_{z^{(L)}} E\left(z^{(L)}\right)
              \circ D_{u^{(L)}} z^{(L)}\left(u^{(L)}\right)
              \circ D_{W^{(L)}} u^{(L)}\left(u^{(L)}\right)
              \right)(H),
              \end{align}
              \begin{align}
              &amp;D_{W^{(L - 1)}} E\left(W^{(L - 1)}\right)(H)
              \\
              &amp;= \left(D_{z^{(L)}} E\left(z^{(L)}\right)
              \circ D_{u^{(L)}} z^{(L)}\left(u^{(L)}\right)
              \circ D_{u^{(L - 1)}} u^{(L)}\left(u^{(L - 1)}\right)
              \circ D_{W^{(L - 1)}} u^{(L - 1)}\left(u^{(L - 1)}\right)
              \right)(H),
              \label{chain}
              \end{align}
              where \(D_x f(x_0)(h)\) denotes the Fréchet derivative of \(f\) at a point \(x_0\) with respect to \(x\)
              along \(h\).

              Calculation of (\ref{chain}) is decomposed into the following calculations:
            <ol class="math">
              <li>Calculate \(D_{z} E(z)(h) \) where \(E\colon \mathbb{R}^n \to \mathbb{R}_+\) is defined as \(\displaystyle E = \frac{1}{2}{\left\|z - y\right\|}^2\).</li>
              <li>Calculate \(D_{u} \sigma(u)(h)\) where \(\sigma\colon \mathbb{R}^n \to \mathbb{R}^n\) is of the form \(\displaystyle \sigma(u) = \begin{pmatrix}\sigma_1(u_1) \\ \vdots \\ \sigma_n(u_n)\end{pmatrix}\).</li>
              <li>Calculate \(D_u (W \circ \sigma) (u)(h) \).</li>
              <li>Calculate \(D_W g(W) (H) \), where \(g(W) = Wz \).</li>
            </ol>
            \begin{align*}
            \end{align*}
            </p>
            <h3>1. \(D_zE(z)(h)\)</h3>
            <p>Note that \({\|U\|}^2 = \langle U,\, U \rangle\) and that \( \langle U,\, V\rangle = \trace (U{}^\top V)\).
              \begin{align*}
              E(z + h) - E(z)
              &amp;= \frac{1}{2}\| (z + h) - y \|^2 - \frac{1}{2}\| z - y \|^2 \\
              &amp;= \langle z + h - y ,\, z + h - y \rangle - \frac{1}{2} {\|z - y\|}^2 \\
              &amp;= \frac{1}{2}{\|z - y\|}^2 + \langle h,\, z - y \rangle + \frac{1}{2}{\|h\|}^2 - \frac{1}{2} {\|z - y\|}^2 \\
              &amp;\leq \langle z - y,\, h \rangle + \frac{1}{2} \|h\|^2
              \end{align*}
              Therefore
              \begin{align}
              D_z E(z)(h) = \langle z - y,\, h \rangle.
              \label{derivative-of-norm}
              \end{align}
            </p>
            <h3>2. \(D_u \sigma(u)(h)\)</h3>
            <p>Since we assume each \(\sigma_i\) is differentiable,
              for any \(\varepsilon &gt; 0\),
              we can take \(h_i\) satisfying
              \begin{align*}
              | \sigma_i(u_i + h_i) - \sigma_i(u_i) - \sigma_i'(u_i) h_i | \leq \varepsilon |h_i|
              \end{align*}
              for all \(i \in \{1,\,\ldots,\,n\}\). Then,
              \begin{align*}
              \left\|
              \sigma(u + h) - \sigma(u) -
              \begin{pmatrix}
              \sigma(u_1) h_1 \\
              \vdots \\
              \sigma (u_n) h_n
              \end{pmatrix}
              \right\|
              &amp;= \sqrt{\sum_{k = 1}^n \left(\sigma_k(u_k + h_k) - \sigma_k(u_k) - \sigma_k(u_k) h_k\right)^2 } \\
              &amp;\leq \sqrt{\sum_{k = 1}^n \varepsilon^2 |h_k|^2 } \\
              &amp;= \varepsilon \|h\|
              \end{align*}
              Defining
              \begin{align*}
              \varSigma(u) =
              \begin{pmatrix}
              \sigma_1(u_1) &amp; \cdots &amp; O \\
              \vdots &amp; \ddots &amp; \vdots \\
              O &amp; \cdots &amp; \sigma_n(u_n)
              \end{pmatrix},
              \quad
              h =
              \begin{pmatrix}
              h_1 \\
              \vdots \\
              h_n
              \end{pmatrix},
              \end{align*}
              we obtain
              \begin{align}
              D_u \sigma(u)(h)
              = \varSigma(u) h
              \label{derivative-of-activation}
              \end{align}
            </p>
            <h3>3. \(D_u (W \circ \sigma) (u)(h)\)</h3>
            <p>Since
              \begin{align*}
              W(u + h) - W(u) = Wh,
              \end{align*}
              we have
              \begin{align*}
              D_u W(u)(h) = Wh.
              \end{align*}
              Along with (\ref{derivative-of-activation}),
              \begin{align}
              D_u (W \circ \sigma)(u)(h) = W \varSigma(u) h.
              \label{derivative-of-wsigma}
              \end{align}
            </p>
            <h3>4. \(D_W g(W) (H) \)</h3>
            <p>
              Since
              \begin{align*}
              g(W + H) - g(W) = (W + H) z - W z = Hz,
              \end{align*}
              we obtain
              \begin{align}
              D_W g(W)(H) = Hz.
              \label{derivative-of-product-right}
              \end{align}
            </p>
            <h3>Untitled</h3>
            <p>Combining
              (\ref{derivative-of-norm}),
              (\ref{derivative-of-activation}),
              (\ref{derivative-of-wsigma}),
              and (\ref{derivative-of-product-right}),
              \begin{align*}
              D_{W^{(L)}} E\left(W^{(L)}\right)(H)
              &amp;=
              \left(D_{z^{(L)}} E\left(z^{(L)}\right)
              \circ D_{u^{(L)}} z^{(L)}\left(u^{(L)}\right)
              \circ D_{W^{(L)}} u^{(L)}\left(u^{(L)}\right)
              \right)(H) \\
              &amp;=
              \left( h \mapsto \left\langle z^{(L)} - y,\, h \right\rangle \right) \circ
              \left( h \mapsto \varSigma^{(L)}(u^{(L)}) \right)
              \circ \left(H \mapsto H z^{(L - 1)}\right)(H) \\
              &amp;=
              \left\langle z^{(L)} - y,\, W^{(L)} \varSigma^{(L)}(u^{(L)}) H z^{(L - 1)} \right\rangle
              \end{align*}
            </p>
            <h3>Untitled 2</h3>
            \begin{align*}
            &amp;D_{W^{(L - 1)}} E\left(W^{(L - 1)}\right)(H)
            \\
            &amp;= \left(D_{z^{(L)}} E\left(z^{(L)}\right)
            \circ D_{u^{(L)}} z^{(L)}\left(u^{(L)}\right)
            \circ D_{u^{(L - 1)}} u^{(L)}\left(u^{(L - 1)}\right)
            \circ D_{W^{(L - 1)}} u^{(L - 1)}\left(u^{(L - 1)}\right)
            \right)(H) \\
            &amp;= \left\langle z^{(L)} - y,\, \cdot \right\rangle \circ
            W^{(L)} \varSigma^{(L)}(u^{(L)}) H z^{(L - 1)}
            \end{align*}
          </section>

        </div>
      </article>

    </main>
    <footer>
      <div class="footer text-center">
        <span>© 2015&ndash;2022 Akira Miyazawa</span>
      </div>

    </footer>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap.native@2.0.15/dist/bootstrap-native-v4.min.js"></script>
    <script src="/static/js/prism.js"></script>
    <script src="/static/js/prism-bash.min.js"></script>
    <script src="/static/js/prism-latex.min.js"></script>
    <script src="/static/js/prism-makefile.min.js"></script>
    <script src="/static/js/prism-java.min.js"></script>
    <script src="/static/js/prism-javascript.min.js"></script>
    <script src="/static/js/prism-scala.min.js"></script>
    <script src="/static/js/prism-markdown.min.js"></script>
    <script src="/static/js/prism-apacheconf.min.js"></script>
    <script src="/static/js/prism-perl.min.js"></script>
    <script src="/static/js/prism-haskell.min.js"></script>
    <script src="/static/js/prism-lua.min.js"></script>
    <script src="https://kit.fontawesome.com/30a99ec99f.js" crossorigin="anonymous"></script>
    <script src="/static/js/main.js"></script>


    <script src="/static/js/mathjax.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=TeX-AMS_HTML"></script>
    </script>

  </div>
</body>

</html>