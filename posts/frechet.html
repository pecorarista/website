<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@pecorarista">
    <meta name="twitter:title" content="Solving Linear Regression Problems Using Fréchet Derivatives — pecorarista.com">
    <meta name="twitter:description" content="">
    <meta name="twitter:image" content="https://pecorarista.com/static/images/avatar.png">
    <link rel="icon" href="/favicon.ico">
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Amiri&family=Judson&family=Noto+Sans+JP:wght@400;700&family=Noto+Serif+JP:wght@400;700&family=Roboto+Mono:wght@400;700&family=Roboto:wght@400;700&display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootswatch@4.5.2/dist/cosmo/bootstrap.min.css" integrity="sha384-5QFXyVb+lrCzdN228VS3HmzpiE7ZVwLQtkt+0d9W43LQMzz4HBnnqvVxKg6O+04d" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism-themes/1.9.0/prism-lucario.min.css" integrity="sha512-Aj5jCt/M5XlTn+Y93TazO+EjpRTiUGMrMuaJOcBcu80Fopd7+LSL094m3pbVTNJxf1hOAMSYI/hONBGjvPWwGQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
<link rel="stylesheet" href="/static/css/main.css"/>

    <title>Solving Linear Regression Problems Using Fréchet Derivatives</title>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <nav class="navbar navbar-expand-lg navbar-dark">
  <div class="container">
    <div class="navbar-header pull-left"><a class="navbar-brand" href="/">pecorarista.com</a></div>
    <div class="navbar-header pull-right">
      <ul class="navbar-nav">
        <li class="nav-item"><a class="nav-link active" href="/resources">Resources</a></li>
      </ul>
    </div>
  </div>
</nav>

      </header>
      <main>
        
<article>
  <div class="container">
    <h1>Solving Linear Regression Problems Using Fréchet Derivatives</h1>
    <div class="info text-muted">
      Posted: 2022-10-07
      
        (Updated: 2022-10-09)
      
    </div>
    
<section>
Suppose we have a linear regression problem,
where input vectors are in \(\mathbb{R}^n\)
and output vectors are in \(\mathbb{R}^m\).
Let \(N\) be the size of dataset.
For each sample \((x^i,\,y^i)\), we write
\begin{gather*}
  x^i =
  \begin{pmatrix}
    x^i_1 \\
    \vdots \\
    x^i_n
  \end{pmatrix},
  \quad
  y^i =
  \begin{pmatrix}
    y^i_1 \\
    \vdots \\
    y^i_m
  \end{pmatrix},
\end{gather*}
and define \(X\) and \(Y\) as
\begin{gather*}
  X =
  \begin{pmatrix}
    x^1 &amp;
    \cdots &amp;
    x^N
  \end{pmatrix}
  =
  \begin{pmatrix}
    x^1_1    &amp; \cdots &amp; x^N_1 \\
    \vdots &amp; \ddots &amp; \vdots\\
    x^1_n    &amp; \cdots &amp; x^N_n \\
  \end{pmatrix}, \\
  Y =
  \begin{pmatrix}
    y^1 &amp;
    \cdots &amp;
    y^N
  \end{pmatrix}
  =
  \begin{pmatrix}
    y^1_1  &amp; \cdots &amp; y^N_1 \\
    \vdots &amp; \ddots &amp; \vdots \\
    y^1_m  &amp; \cdots &amp; y^N_m
  \end{pmatrix}.
\end{gather*}
Let \(W \in \mathbb{R}^{m \times n}\)
be the matrix of coefficients as follows:
\begin{gather*}
  W =
  \begin{pmatrix}
    w_{11}  &amp; \cdots &amp; w_{1n} \\
    \vdots  &amp; \ddots &amp; \vdots \\
    w_{m1}  &amp; \cdots &amp; w_{mn}
  \end{pmatrix}. \\
\end{gather*}
The squared error \(E\) is
\begin{align*}
  E(W) = \frac{1}{2} \| WX - Y \|^2,
\end{align*}
where \(\|\cdot\|\) is the Frobenius norm.</p>
<p>If we change \(W\) along \(H\),
\(E\) changes as below:
\begin{align}
  E(W + H) - E(H)
  &amp;= \frac{1}{2}{
    \left\|
      (W + H)X - Y
    \right\|
  }^2
  -
  \frac{1}{2}
  {
    \left\|
      WX - Y
    \right\|
  }^2 \notag \\
  &amp;=
  \frac{1}{2}
  \langle
    WX - Y + HX,\,
    WX - Y + HX
  \rangle
  -
  \frac{1}{2}
  {
    \left\|
      WX - Y
    \right\|
  }^2 \notag \\
  &amp;=
  \frac{1}{2}
  {\| WX - Y \|}^2
  +
  \langle
    WX - Y,\,
    HX
  \rangle
  +
  \frac{1}{2}
  {\|
    HX
  \|}^2
  -
  \frac{1}{2}
  {\|
    WX - Y
  \|}^2 \notag \\
  &amp;=
  \trace (
    (WX - Y) (HX)^\top
  )
  +\frac{1}{2}
  {\|HX\|}^2 \notag \\
  &amp;=
  \trace(
    (WX - Y)X^\top H^\top
  )
  +\frac{1}{2}
  {\|HX\|}^2 \notag \\
  &amp;=
  \langle
    (WX - Y)X^\top,\,
    H
  \rangle
  + \frac{1}{2}{\|HX\|}^2 \label{result}
\end{align}
According to Cauchy&ndash;Schwarz inequality, \(\|HX\|^2 \leq {\|H\|}^2 {\|X\|}^2 \).
Therefore,
for any \(\varepsilon &gt; 0\),
if we take \(H\) so small that
\(\|H\| \leq \displaystyle \frac{\varepsilon}{2 \|X\|^2 + 1},\)
\begin{align*}
  \|f(W + H) - f(W) - \langle ( WX - Y)X^\top,\, H \rangle\| \leq \varepsilon \|H\|
\end{align*}
The derivative of \(f\) with respect to \(W\) at an arbitrary point \(W\) along \(H\)
is the first term of the right-hand side of (\ref{result}):
\begin{align*}
  D_W f(W)(H) = \langle ( WX - Y )X^\top,\, H \rangle.
\end{align*}
Since \(f\) is convex,
\(f\) has the global minimum if and only if
\begin{gather*}
  ( WX - Y )X^\top = 0 \\
  W XX^\top = YX^\top
\end{gather*}
If \(XX^\top\) is regular,
\begin{gather*}
  W = Y X^\top (XX^\top)^{-1}.
\end{gather*}
</p>
</section>

  </div>
</article>

      </main>
      <footer>
        <div class="footer text-center">
  <span>© 2015&ndash;2022 Akira Miyazawa</span>
</div>

      </footer>
      <script src="https://cdn.jsdelivr.net/npm/bootstrap.native@2.0.15/dist/bootstrap-native-v4.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha512-SkmBfuA2hqjzEVpmnMt/LINrjop3GKWqsuLSSB3e7iBmYK7JuWw4ldmmxwD9mdm2IRTTi0OxSAfEGvgEi0i2Kw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="https://kit.fontawesome.com/30a99ec99f.js" crossorigin="anonymous"></script>
<script src="/static/js/main.js"></script>

      
        <script src="/static/js/mathjax.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=TeX-AMS_HTML"></script>
        </script>
      
    </div>
  </body>
</html>
