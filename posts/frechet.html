<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@pecorarista">
  <meta name="twitter:title" content="Solving Linear Regression Problems Using Fréchet Derivatives — pecorarista.com">
  <meta name="twitter:description" content="">
  <meta name="twitter:image" content="https://pecorarista.com/static/images/avatar.png">
  <link rel="icon" href="/favicon.ico">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP&family=Noto+Serif+JP&display=swap">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto&display=swap">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Amiri&display=swap">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Judson&display=swap">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&display=swap">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootswatch@4.5.2/dist/cosmo/bootstrap.min.css" integrity="sha384-5QFXyVb+lrCzdN228VS3HmzpiE7ZVwLQtkt+0d9W43LQMzz4HBnnqvVxKg6O+04d" crossorigin="anonymous">
  <link rel="stylesheet" href="/static/css/prism-lucario.css" />
  <link rel="stylesheet" href="/static/css/main.css" />

  <title>Solving Linear Regression Problems Using Fréchet Derivatives</title>
</head>

<body>
  <div class="wrapper">
    <header>
      <nav class="navbar navbar-expand-lg navbar-dark">
        <div class="container">
          <div class="navbar-header pull-left"><a class="navbar-brand" href="/">pecorarista.com</a></div>
          <div class="navbar-header pull-right">
            <ul class="navbar-nav">
              <li class="nav-item"><a class="nav-link active" href="/resources">Resources</a></li>
            </ul>
          </div>
        </div>
      </nav>

    </header>
    <main>

      <article>
        <div class="container">
          <h1>Solving Linear Regression Problems Using Fréchet Derivatives</h1>
          <div class="info text-muted">
            Posted: 2022-10-07

            (Updated: 2022-10-09)

          </div>

          <section>
            Suppose we have a linear regression problem,
            where input vectors are in \(\mathbb{R}^n\)
            and output vectors are in \(\mathbb{R}^m\).
            Let \(N\) be the size of dataset.
            For each sample \((x^i,\,y^i)\), we write
            \begin{gather*}
            x^i =
            \begin{pmatrix}
            x^i_1 \\
            \vdots \\
            x^i_n
            \end{pmatrix},
            \quad
            y^i =
            \begin{pmatrix}
            y^i_1 \\
            \vdots \\
            y^i_m
            \end{pmatrix},
            \end{gather*}
            and define \(X\) and \(Y\) as
            \begin{gather*}
            X =
            \begin{pmatrix}
            x^1 &amp;
            \cdots &amp;
            x^N
            \end{pmatrix}
            =
            \begin{pmatrix}
            x^1_1 &amp; \cdots &amp; x^N_1 \\
            \vdots &amp; \ddots &amp; \vdots\\
            x^1_n &amp; \cdots &amp; x^N_n \\
            \end{pmatrix}, \\
            Y =
            \begin{pmatrix}
            y^1 &amp;
            \cdots &amp;
            y^N
            \end{pmatrix}
            =
            \begin{pmatrix}
            y^1_1 &amp; \cdots &amp; y^N_1 \\
            \vdots &amp; \ddots &amp; \vdots \\
            y^1_m &amp; \cdots &amp; y^N_m
            \end{pmatrix}.
            \end{gather*}
            Let \(W \in \mathbb{R}^{m \times n}\)
            be the matrix of coefficients as follows:
            \begin{gather*}
            W =
            \begin{pmatrix}
            w_{11} &amp; \cdots &amp; w_{1n} \\
            \vdots &amp; \ddots &amp; \vdots \\
            w_{m1} &amp; \cdots &amp; w_{mn}
            \end{pmatrix}. \\
            \end{gather*}
            The squared error \(E\) is
            \begin{align*}
            E(W) = \frac{1}{2} \| WX - Y \|^2,
            \end{align*}
            where \(\|\cdot\|\) is the Frobenius norm.</p>
            <p>If we change \(W\) along \(H\),
              \(E\) changes as below:
              \begin{align}
              E(W + H) - E(H)
              &amp;= \frac{1}{2}{
              \left\|
              (W + H)X - Y
              \right\|
              }^2
              -
              \frac{1}{2}
              {
              \left\|
              WX - Y
              \right\|
              }^2 \notag \\
              &amp;=
              \frac{1}{2}
              \langle
              WX - Y + HX,\,
              WX - Y + HX
              \rangle
              -
              \frac{1}{2}
              {
              \left\|
              WX - Y
              \right\|
              }^2 \notag \\
              &amp;=
              \frac{1}{2}
              {\| WX - Y \|}^2
              +
              \langle
              WX - Y,\,
              HX
              \rangle
              +
              \frac{1}{2}
              {\|
              HX
              \|}^2
              -
              \frac{1}{2}
              {\|
              WX - Y
              \|}^2 \notag \\
              &amp;=
              \trace (
              (WX - Y) (HX)^\top
              )
              +\frac{1}{2}
              {\|HX\|}^2 \notag \\
              &amp;=
              \trace(
              (WX - Y)X^\top H^\top
              )
              +\frac{1}{2}
              {\|HX\|}^2 \notag \\
              &amp;=
              \langle
              (WX - Y)X^\top,\,
              H
              \rangle
              + \frac{1}{2}{\|HX\|}^2 \label{result}
              \end{align}
              According to Cauchy&ndash;Schwarz inequality, \(\|HX\|^2 \leq {\|H\|}^2 {\|X\|}^2 \).
              Therefore,
              for any \(\varepsilon &gt; 0\),
              if we take \(H\) so small that
              \(\|H\| \leq \displaystyle \frac{\varepsilon}{2 \|X\|^2 + 1},\)
              \begin{align*}
              \|f(W + H) - f(W) - \langle ( WX - Y)X^\top,\, H \rangle\| \leq \varepsilon \|H\|
              \end{align*}
              The derivative of \(f\) with respect to \(W\) at an arbitrary point \(W\) along \(H\)
              is the first term of the right-hand side of (\ref{result}):
              \begin{align*}
              D_W f(W)(H) = \langle ( WX - Y )X^\top,\, H \rangle.
              \end{align*}
              Since \(f\) is convex,
              \(f\) has the global minimum if and only if
              \begin{gather*}
              ( WX - Y )X^\top = 0 \\
              W XX^\top = YX^\top
              \end{gather*}
              If \(XX^\top\) is regular,
              \begin{gather*}
              W = Y X^\top (XX^\top)^{-1}.
              \end{gather*}
            </p>
          </section>

        </div>
      </article>

    </main>
    <footer>
      <div class="footer text-center">
        <span>© 2015&ndash;2022 Akira Miyazawa</span>
      </div>

    </footer>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap.native@2.0.15/dist/bootstrap-native-v4.min.js"></script>
    <script src="/static/js/prism.js"></script>
    <script src="/static/js/prism-bash.min.js"></script>
    <script src="/static/js/prism-latex.min.js"></script>
    <script src="/static/js/prism-makefile.min.js"></script>
    <script src="/static/js/prism-java.min.js"></script>
    <script src="/static/js/prism-javascript.min.js"></script>
    <script src="/static/js/prism-scala.min.js"></script>
    <script src="/static/js/prism-markdown.min.js"></script>
    <script src="/static/js/prism-apacheconf.min.js"></script>
    <script src="/static/js/prism-perl.min.js"></script>
    <script src="/static/js/prism-haskell.min.js"></script>
    <script src="/static/js/prism-lua.min.js"></script>
    <script src="https://kit.fontawesome.com/30a99ec99f.js" crossorigin="anonymous"></script>
    <script src="/static/js/main.js"></script>


    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <script>
      MathJax = {
        loader: {
          load: [
            '[tex]/cases',
            '[tex]/empheq'
          ]
        },
        tex: {
          packages: {
            '[+]': [
              'empheq',
              'cases'
            ]
          },
          autoload: {
            cases: [
              [],
              ['numcases', 'subnumcases']
            ]
          },
          macros: {
            abs: ["\\left|#1\\right|", 1],
            brackets: ["\\left[#1\\right]", 1],
            diag: "\\mathop{\\rm diag}",
            ff: "\\mathop{\\rm FF}",
            id: "\\mathop{\\rm id}",
            imaginary: "\\mathop{\\rm Im}",
            minimize: "\\mathop{\\rm minimize}",
            nn: "\\mathop{\\rm NN}",
            norm: ["\\left\\|#1\\right\\|", 1],
            parentheses: ["\\left(#1\\right)", 1],
            real: "\\mathop{\\rm Re}",
            res: "\\mathop{\\rm Res}",
            relu: "\\mathop{\\rm ReLU}",
            trace: "\\mathop{\\rm tr}",
            softmax: "\\mathop{\\rm softmax}"
          },
          tags: 'ams',
        }
      };
    </script>

  </div>
</body>

</html>