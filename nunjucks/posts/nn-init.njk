{% extends '_layout/_article.njk' %}
{% set lang = 'ja' %}
{% set mathematical = 'true' %}
{% set title = 'ニューラルネットの線型層の初期化' %}
{% set posted = '2022-09-25' %}

{% block post %}
<section>
<p><strong>【前提】</strong>
各層の入出力の分散が一定であることが望ましい．</p>
<p>ニューラルネットの第
\(\ell\)
番目の線型層について，
入力の次元を
\(n_\ell\)，
出力の次元を
\(n_{\ell + 1}\)，
入力と重みをそれぞれ
\begin{align*}
  x^{(\ell)} =
    \begin{pmatrix}
      x^{(\ell)}_1 \\
      \vdots \\
      x^{(\ell)}_{n_\ell}
    \end{pmatrix},\quad
  W^{(\ell)} =
    \begin{pmatrix}
      w_{11}^{(\ell)}       &amp; \cdots &amp; w_{1 n_\ell}^{(\ell)} \\
      \vdots                &amp; \ddots &amp; \vdots \\
      w_{n_{\ell + 1} 1}^{(\ell)} &amp; \cdots &amp; w_{n_{\ell + 1} n_\ell}^{(\ell)}
    \end{pmatrix}
\end{align*}
とする．
各
\(w_{ij}^{(\ell)}\)
の各成分は
期待値
\(0
\)
の同一の分布から独立に選ぶ．
このとき任意の
\(i,\,j,\,k\)
について
\(x_k\)
と
\(w_{ij}\)
は独立になる．
線型層の出力
\(z^{(\ell)} := W^{(\ell)}x^{(\ell)}\)
の第
\(i\)
成分の分散は，
\(\mathcal{J}_{n_\ell} := \{1,\,\ldots,\,n_{\ell}\}\)
と表すことにすると
\begin{align}
  V\left[z^{(\ell)}_i\right]
    &amp;= V \left[
      \sum_{j \in \mathcal{J}_{n_\ell}} w_{ij}^{(\ell)} x_j^{(\ell)}
    \right] \notag \\
    &amp;=
      E\left[
        {
        \left(
          \sum_{j \in \mathcal{J}_{n_\ell}}
          w_{ij}^{(\ell)} x_i^{(\ell)}
        \right)
        }^2
      \right]
      -
      {
        \left(
          E\left[
            \sum_{j \in \mathcal{J}_{n_\ell}}
              w_{ij}^{(\ell)} x_i^{(\ell)}
          \right]
        \right)
      }^2 \label{a}
\end{align}
となる．</p>
<p>(\ref{a})の右辺第1項について
\begin{align}
      E\left[
        {
        \left(
          \sum_{j \in \mathcal{J}_{n_\ell}}
          w_{ij}^{(\ell)} x_i^{(\ell)}
        \right)
        }^2
      \right]
    &amp;=
    \sum_{(j,\, j') \in \mathcal{J}_{n_\ell} \times \mathcal{J}_{n_\ell}}
      E\left[w_{ij}^{(\ell)} x_{j}^{(\ell)} w_{ij'}^{(\ell)} x_{j'}^{(\ell)}\right] \notag \\
    &amp;=
      \sum_{j \in \mathcal{J}_{n_\ell}}
      E\left[
        {
          \left(
            w_{ij}^{(\ell)}
          \right)
        }^2
      \right]
      E\left[{\left(x_j^{(\ell)}\right)}^2 \right]
      +
      \sum_{
        \substack{
          (j,\, j') \in \mathcal{J}_{n_\ell} \times \mathcal{J}_{n_\ell} \\
          j \neq j'
        }
      }
      E\left[w_{ij}^{(\ell)}\right]
      E \left[w_{ij'}^{(\ell)} \right]
      E\left[x_j^{(\ell)} x_{j'}^{(\ell)} \right] \notag \\
    &amp;=
      \sum_{j \in \mathcal{J}_{n_\ell}}
      \left(V\left[w_{ij}\right] - \left(E[w_{ij}]\right)^2\right)
      E \left[{\left(x_j^{(\ell)}\right)}^2\right]
      +
      \sum_{
        \substack{
          (j,\, j') \in \mathcal{J}_{n_\ell} \times \mathcal{J}_{n_\ell} \\
          j \neq j'
        }
      } 0 \cdot 0 \cdot E\left[x_j^{(\ell)} x_{j'}^{(\ell)} \right]
      \notag \\
    &amp;=
      V_{W}^{(\ell)}
      \sum_{j \in \mathcal{J}_{n_\ell}}
      E \left[{\left(x_j^{(\ell)}\right)}^2\right]. \label{w1}
\end{align}
ただし
\(V_{W}^{(\ell)} = V\left[w_{i1}^{(\ell)}\right] = \cdots V\left[w_{n_\ell}^{(\ell)}\right]\)
と置いた．</p>
<p>(\ref{a})の右辺第2項について
\begin{align}
    {\left(\sum_{j \in \mathcal{J}_{n_\ell}} E\left[w_{ij}^{(\ell)}\right]E\left[x_j^{(\ell)}\right]\right)}^2
    =
    {\left(\sum_{j \in \mathcal{J}_{n_\ell}} 0 \cdot E\left[x_j^{(\ell)}\right]\right)}^2 = 0 \label{w2}
\end{align}
が成り立つ．(\ref{w1})と(\ref{w2})を合わせて
\begin{align}
  V\left[z_i^{(\ell)}\right]
  =
    V_{W}^{(\ell)}
      \sum_{j \in \mathcal{J}_{n_\ell}}
      E \left[{\left(x_j^{(\ell)}\right)}^2\right].
      \label{v}
\end{align}
という式を得る．</p>
<p>
活性化関数を
\(f\)
として
\(u_i^{(\ell)} := f(z_i^{(\ell)})\)
と定める．
以下の仮定をおく．
\begin{gather*}
  V\left[u_i^{(\ell)}\right]
    = V\left[z_i^{(\ell)}\right]
    \text{ for all \(i \in \mathcal{J}_{n_{\ell + 1}}\)}, \\
  E\left[x_j^{(\ell)}\right]
    = 0
    \text{ for all \(j \in \mathcal{J}_{n_\ell}\)}, \\
  V\left[u_i^{(\ell)}\right]
    = V\left[x_j^{(\ell)}\right]
    \text{ for all \((i,\, j) \in \mathcal{J}_{n_{\ell + 1}} \times \mathcal{J}_{n_\ell}\).}
\end{gather*}
これら仮定の下で(\ref{v})より
\begin{gather}
  V\left[u_i^{(\ell)}\right]
  =
  V_{W}^{(\ell)}
  \sum_{j \in \mathcal{J}_{n_\ell}} V\left[x_j^{(\ell)}\right]
  \notag \\
  V\left[u_i^{(\ell)}\right]
  =
  n_{\ell}
  V_{W}^{(\ell)}
  V\left[x_{1}^{(\ell)}\right] \notag \\
  n_{\ell} V_{W}^{(\ell)} = 1. \label{n}
\end{gather}
同様にして，逆伝播のときは
\begin{align}
n_{\ell + 1} V_{W}^{(\ell)} = 1. \label{n1}
\end{align}
(\ref{n})と(\ref{n1})を同時に満たすような
\(V_{W}^{(\ell)}\)
をとりたいが，そのような
\(V_{W}^{(\ell)}\)
は一般には存在しないため，妥協案として
\begin{align}
  V_W^{(\ell)} = \frac{2}{n_{\ell} + n_{\ell + 1}} \label{result}
\end{align}
と定める．これは
\(n_{\ell} = n_{\ell + 1}\)
が成り立つとき，(\ref{n})と(\ref{n1})を満たす．</p>
<p><strong>【例】</strong>
\(W^{\ell}\)の各要素が
\(\mathcal{U}(-a,\,a)\)
（
\(a\)
は正の定数）から選ばれる場合を考える．このとき各要素の分散
\(V^{(\ell)}_W\)
は
\begin{align*}
  V^{(\ell)}_W = \frac{(a - (- a))^2}{12} = \frac{a^2}{3}.
\end{align*}
で与えられる．(\ref{result})を利用すると
\begin{align*}
  \frac{a^2}{3} = \frac{2}{n_\ell + n_{\ell + 1}}.
\end{align*}
\(a\)
は正だったことから
\begin{align*}
  a = \sqrt{\frac{6}{n_\ell + n_{\ell + 1}}}.
\end{align*}
したがって
\(W^{(\ell)}\)
の各要素は
\begin{gather*}
  \mathcal{U}\left(
    - \sqrt{\frac{6}{n_{\ell} + n_{\ell + 1}}},\,
   \sqrt{\frac{6}{n_{\ell} + n_{\ell + 1}}}
  \right)
\end{gather*}
からサンプリングすればよい．</p>
</section>
<section>
<h2>参考文献</h2>
  <ul>
  <li>Glorot, X., &amp; Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In <i>Proceedings of the thirteenth international conference on artificial intelligence and statistics</i> (pp. 249–256). JMLR Workshop and Conference Proceedings.</li>
    <li>岡谷貴之(2022)．『深層学習 改訂第2版（機械学習プロフェッショナルシリーズ）』．講談社．</li>
  </ul>
</section>
{% endblock %}
