{% extends '_layout/_article.njk' %}
{% set lang = 'en' %}
{% set mathematical = 'true' %}
{% set title = 'Computing Backpropagation Using Fréchet Derivative' %}
{% set posted = '2022-10-09' %}

{% block post %}
<section>
<p><h3><em>This page is a draft.</em></h3></p>
</section>
<section>
<h2>Simple Example 1</h2>
<p>
Let \(W \in \mathbb{R}^{m \times n}\) be the final linear layer of a multi-layer neural network,
\(z \in \mathbb{R}^n\) be the output of the previous layer
computed from an arbitrary input vector \(x_i \in \mathbb{R}^n\),
and \(y = y_i \in \mathbb{R}^m\) be the target vector corresponding to \(x_i \in \mathbb{R}^n\).
The activation function of the last layer is the identity function for simplicity.
</p>
<p>The loss function \(E\) is defined as
\begin{align}
  E(W) = \frac{1}{2}\|Wz - y\|^2.
\end{align}
To derive Fréchet deriative of \(E\),
we first compute \(E(W + H) - E(W)\).
\begin{align*}
  &amp; E(W + H) - E(W) \\
  &amp;= \frac{1}{2}\|(W + H)z - y\|^2 - \frac{1}{2}\|Wz - y\|^2 \\
  &amp;= \frac{1}{2} \langle Wz + Hz - y,\, Wz + Hz - y \rangle
  -\frac{1}{2}{\| Wz - y \|}^2 \\
  &amp;= \frac{1}{2}{\| Wz - y \|}^2
  + \langle Wz - y,\, Hz \rangle
  + \frac{1}{2}{\|Hz\|}^2
  -\frac{1}{2}{\| Wz - y \|}^2 \\
  &amp;= \trace ((Wz - y)z^\top H^\top )+ \frac{1}{2} {\|Hz\|}^2 \\
  &amp;= \langle (Wz - y)z^\top,\, H \rangle + \frac{1}{2} {\|Hz\|}^2.
\end{align*}
By using Cauchy&ndash;Schwarz inequality,
\begin{align*}
  \frac{1}{2} {\|Hz\|}^2 \leq \frac{1}{2} \|H\|^2 \|z\|^2.
\end{align*}
Therefore, for any \(\varepsilon &gt; 0\),
if we take \(H\) satisfying \(\displaystyle \|H\| \leq \frac{\varepsilon}{2 \|z\|^2 + 1}\),
\begin{align*}
  | E(W + H) - E(W) - \langle (Wz - y)z^\top,\, H \rangle | \leq \varepsilon \| H \|.
\end{align*}
Thus we have
\begin{align}
  DE(W)(H) = \langle (Wz - y)z^\top,\, H \rangle. \label{dewzy}
\end{align}
</p>
<p>We can have (\ref{dewzy}) in another way using the chain rule:
\begin{align*}
  D(g \circ f) = Dg(f(x)) \circ Df(x).
\end{align*}
Define \(g\colon \mathbb{R}^{m} \to \mathbb{R}\)
and \(f\colon \mathbb{R}^{m \times n} \to \mathbb{R}^m\) as follows:
\begin{align*}
  g(u) = \frac{1}{2}\|u - y\|^2,\quad f(W) = Wz.
\end{align*}
Then
\begin{align*}
  g(z + h) - g(z)
  &amp;= \frac{1}{2}\|u + h - y\|^2 - \frac{1}{2}\|z - y\|^2 \\
  &amp;= \frac{1}{2}\langle u + h - y,\, u + h - y\rangle - \frac{2}{2}\| u - y \|^2 \\
  &amp;= \frac{1}{2} \| u - y \|^2 + \langle u - y,\, h\rangle - \frac{1}{2}\| u - y \|^2 \\
  &amp;= \langle u - y,\, h\rangle,
\end{align*}
and
\begin{align*}
  f(W + H) - f(W)
  &amp;= (W + H)z - Wz \\
  &amp;= Hz.
\end{align*}
Therefore
\begin{align*}
  D(g \circ f)(Wz)(H) = \langle Wz - y,\, Hz \rangle = \langle (Wz - y)z^\top,\, H \rangle.
\end{align*}
</p>
<h3>Elements of the Derivative</h3>
<p>To compute
\(\partial E / \partial w_{ij}\),
define
\(H_{ij} \in \mathbb{R}^{m \times n}\)
as follows:
\begin{gather*}
  \begin{array}{c}
      \begin{array}{ccccccc}
                  &amp;        &amp;   &amp; \text{\(j\)-th} &amp;   &amp;        &amp; \\
                  H_{ij} = hE_{ij},\quad
      E_{ij} = (0 &amp; \cdots &amp; 0 &amp; he_i &amp; 0   &amp; \cdots &amp; 0),\quad
      \end{array}
  \end{array}
  \begin{array}{c}
      \begin{array}{ccccccc}
               &amp;        &amp;   &amp; \text{\(i\)-th} &amp;   &amp;        &amp; \\
      e_i = (0 &amp; \cdots &amp; 0 &amp; 1 &amp; 0   &amp; \cdots &amp; 0)^\top.
      \end{array}
  \end{array}
\end{gather*}
Let \(A = (Wz - y)z^\top\). Then,
\begin{align*}
  h \frac{\partial E}{\partial w_{ij}}
  &amp;= \langle A,\, H_{ij} \rangle \\
  &amp;= h \trace ( A^\top E_{ij} ) \\
  &amp;\;
  \begin{array}{c}
    \begin{array}{ccccccc}
      &amp;        &amp;   &amp; \text{\(j\)-th} &amp;   &amp;        &amp; \\
      = h\trace (0 &amp; \cdots &amp; 0 &amp; (\text{\(A\)'s \(i\)-th row})^\top &amp; 0   &amp; \cdots &amp; 0),
    \end{array}
  \end{array} \\
  &amp;= h a_{ij}
\end{align*}
Thus we have
\begin{align*}
  \frac{\partial E}{\partial w_{ij}} &amp;= a_{ij},
\end{align*}
and the partial derivative of \(E\) with respect to \(w_{ij}\) is
\begin{align*}
  \frac{\partial E}{\partial w_{ij}} = [(Wz - y)z^\top]_{ij},
\end{align*}
where \([A]_{ij}\) is the \((i,\,j)\)-the element of a matrix \(A\).
</p>
</section>
<section>
<h2>Simple Example 2</h2>
<p>
\(W \in \mathbb{R}^{m \times n}\)
\begin{gather}
E(W) = \frac{1}{2}\|u' - y\|,\\
u' = W'z, \\
z = \sigma(u),\\
u = Wx.
\end{gather}
Here \(\sigma\) is an activation function of the form
\begin{align*}
  \sigma(x) =
    \begin{pmatrix}
      \sigma_1(x_1) \\
      \vdots \\
      \sigma_n(x_n) \\
      \end{pmatrix}.
\end{align*}
We suppose each \(\sigma_i\) is differentiable;
for any \(\varepsilon &gt; 0\),
if we take \(h_i\) whose norm is sufficiently small, it holds that
\begin{align*}
  | \sigma_i(x_i + h_i) - \sigma_i(x_i) - \sigma_i'(x_i)  h_i  | \leq \varepsilon |h_i|
\end{align*}
for all \(i \in \{1,\,\ldots,\,n\}\). Then,
\begin{align*}
\left\|
  \sigma(x + h) - \sigma(x) -
    \begin{pmatrix}
      \sigma(x_1) h_1 \\
      \vdots \\
      \sigma (x_n) h_n
    \end{pmatrix}
\right\|
&amp;= \sqrt{\sum_{k = 1}^n \left(\sigma_k(x_k + h_k)  - \sigma_k(x_k) - \sigma_k(x_k) h_k\right)^2 } \\
&amp;\leq \sqrt{\sum_{k = 1}^n \varepsilon^2 |h_k|^2 }  \\
&amp;= \varepsilon \|h\|
\end{align*}
Therefore we have
\begin{align*}
  D\sigma(x)(h)
  &amp;=
    \begin{pmatrix}
      \sigma'(x_1) h_1 \\
      \vdots \\
      \sigma'(x_n) h_n
    \end{pmatrix} \\
  &amp;= \begin{pmatrix}
      \sigma_1'(x_1) &amp; \cdots &amp; O \\
      \vdots &amp; \ddots &amp; \vdots \\
      O &amp; \cdots &amp; \sigma_n'(x_n)
    \end{pmatrix}
    \begin{pmatrix}
      h_1 \\
      \vdots \\
      h_n
    \end{pmatrix}.
\end{align*}
</p>
</section>
{% endblock %}
