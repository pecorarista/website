{% extends '_layout/_article.njk' %}
{% set lang = 'ja' %}
{% set mathematical = 'true' %}
{% set title = 'Calculating Backpropagation Using Fréchet Derivative' %}
{% set posted = '2022-10-09' %}

{% block post %}
<section>
<p>This post is those who have trouble understanding backpropagation</p>
<p>
Consider a multilayer perceptron consisting of \(L\) layers, whose \(\ell\)-th layer has \(n(\ell)\) nodes.
The loss function is the squared error
\begin{align*}
  E(z) =
  \frac{1}{2}
  {
    \left\|
      z  - y
    \right\|
  }^2,
\end{align*}
and the activation function for the \(\ell\)-th layer is
\(\sigma^{(\ell)}\colon \mathbb{R}^{n(\ell)} \to \mathbb{R}^{n(\ell)}\)
and
\(\sigma^{(\ell)}_i\colon \mathbb{R} \to \mathbb{R}\)
The activation function is expressed as
\begin{gather*}
  \sigma^{(\ell)}(x) =
    \begin{pmatrix}
      \sigma_1^{(\ell)}(x_1) \\
      \vdots \\
      \sigma_{n(\ell)}^{(\ell)}\left(x_{n(\ell)}\right)
    \end{pmatrix},\\[5pt]
\end{gather*}
where each \(\sigma^{(\ell)}_i\colon \mathbb{R} \to \mathbb{R}\) is differentiable.
Let \(u^{(\ell)} \in \mathbb{R}^{n(\ell)}\) be the input to the \(\ell\)-th layer
and \(z^{(\ell)} \in \mathbb{R}^{n(\ell)}\) be the output from the \(\ell\)-th layer:
\begin{align*}
  z^{(\ell)} = \sigma^{(\ell)} \left( u^{(\ell)} \right).
\end{align*}
The weight of the \(\ell\)-th layer is
\(W^{(\ell)} \in \mathbb{R}^{n(\ell) \times n(\ell - 1)}\), which is expressed as
\begin{align*}
  W^{(\ell)} =
  \begin{pmatrix}
    w_{1,\,1}^{(\ell)}       &amp; \cdots &amp; w_{1,\,n(\ell - 1)}^{(\ell)} \\
    \vdots                   &amp; \ddots &amp; \vdots \\
    w_{n(\ell),\,1}^{(\ell)} &amp; \cdots &amp; w_{n(\ell),\,n(\ell - 1)}
  \end{pmatrix},
\end{align*}
so this means that
\begin{align*}
  u^{(\ell)} = W^{(\ell)} z^{(\ell - 1)} = W^{(\ell)} \sigma^{(\ell - 1)} \left( u^{(\ell - 1)} \right),\quad
\end{align*}
I use the symbol \(W\) for the linear map \(z^{(\ell - 1)} \mapsto W^{(\ell) } z^{(\ell - 1)}\) as well.
<figure class="centered">
  <figcaption><strong>Figure 1:</strong> Architecture of the neural network.</figcaption>
  <img
    src="/static/images/nn.svg"
    alt="Architecture of the neural network"
    style="background-color: transparent; min-width: 700px; margin-top: 2ex;"
  />
</figure>
</p>
<p>
The goal of this post is to derive the derivative of \(E\) with respect to \(W\)
because other cases are easily derived in similar ways.
First, assume each variable is one-dimensional to aid intuitive understanding.
Then,
\begin{align*}
  \frac{dE}{dW^{(L - 1)}} =
    \frac{dE}{dz^{(L)}}
    \frac{dz^{(L)}}{du^{(L)}}
    \frac{du^{(L)}}{du^{(L - 1)}}
    \frac{du^{(L - 1)}}{dW^{(L - 1)}},
    \label{one-dim-chain}
\end{align*}
In general cases where dimensions are greater than one, the following analogous formula holds
\begin{align}
  &amp; D_z E\left(W^{(L - 1)}\right)(H) \notag \\
  &amp;=
  \left(D_{z^{(L)}} E\left(z^{(L)}\right)
  \circ D_{u^{(L)}} z^{(L)}\left(u^{(L)}\right)
  \circ D_{u^{(L - 1)}} u^{(L)}\left(u^{(L - 1)}\right)
  \circ  D_{W^{(L - 1)}} u^{(L - 1)}
  \right)(H)
  \label{chain}
\end{align}
where \(D_x f(x_0)(h)\) denotes the Fréchet derivative of \(f\) at a point \(x_0\) with respect to \(x\)
along \(h\).

Calculation of (\ref{chain}) is decomposed into the following calculations:
<ol class="math">
  <li>Calculate \(D_{z} E(z)(h) \) where \(E\colon \mathbb{R}^n \to \mathbb{R}_+\) is defined as \(\displaystyle E = \frac{1}{2}{\left\|z - y\right\|}^2\).</li>
  <li>Calculate \(D_{u} \sigma(u)(h)\) where \(\sigma\colon \mathbb{R}^n \to \mathbb{R}^n\) is of the form \(\displaystyle \sigma(u) = \begin{pmatrix}\sigma_1(u_1) \\ \vdots \\ \sigma_n(u_n)\end{pmatrix}\).</li>
  <li>Calculate \(D_u (W \circ \sigma) (u)(h) \).</li>
  <li>Calculate \(D_W W (u)(h) \).</li>
</ol>
\begin{align*}
\end{align*}
</p>
<h3>1. \(D_zE(z)(h)\)</h3>
<p>For any \(\varepsilon &gt; 0\),
if we take \(h\) so small that it satisfies\(\displaystyle \|h\| \leq 2\varepsilon\),
\begin{align*}
  E(z + h) - E(z)
  &amp;= \frac{1}{2}\| (z + h) - y \|^2 - \frac{1}{2}\| z - y \|^2 \\
  &amp;= \langle z + h - y ,\, z + h - y \rangle - \frac{1}{2} {\|z - y\|}^2 \\
  &amp;= \frac{1}{2}{\|z - y\|}^2 + \langle h,\, z - y \rangle + \frac{1}{2}{\|h\|}^2 - \frac{1}{2} {\|z - y\|}^2 \\
  &amp;\leq \langle z - y,\, h \rangle + \varepsilon{\|h\|}.
\end{align*}
Therefore
\begin{align}
  D_z E(z)(h) = \langle z - y,\, h \rangle.
\end{align}
</p>
<h3>2. \(D_u \sigma(u)(h)\)</h3>
<p>
\begin{align*}
  \sigma(u) =
    \begin{pmatrix}
      \sigma_1(u_1) \\
      \vdots \\
      \sigma_n(u_n) \\
      \end{pmatrix}.
\end{align*}
We suppose each \(\sigma_i\) is differentiable;
for any \(\varepsilon &gt; 0\),
if we take \(h_i\) whose norm is sufficiently small, it holds that
\begin{align*}
  | \sigma_i(u_i + h_i) - \sigma_i(u_i) - \sigma_i'(u_i)  h_i  | \leq \varepsilon |h_i|
\end{align*}
for all \(i \in \{1,\,\ldots,\,n\}\). Then,
\begin{align*}
\left\|
  \sigma(u + h) - \sigma(u) -
    \begin{pmatrix}
      \sigma(u_1) h_1 \\
      \vdots \\
      \sigma (u_n) h_n
    \end{pmatrix}
\right\|
&amp;= \sqrt{\sum_{k = 1}^n \left(\sigma_k(u_k + h_k)  - \sigma_k(u_k) - \sigma_k(u_k) h_k\right)^2 } \\
&amp;\leq \sqrt{\sum_{k = 1}^n \varepsilon^2 |h_k|^2 }  \\
&amp;= \varepsilon \|h\|
\end{align*}
Therefore we have
\begin{align}
  D\sigma(x)(h)
  &amp;=
    \begin{pmatrix}
      \sigma(u_1) h_1 \\
      \vdots \\
      \sigma(u_n) h_n
    \end{pmatrix} \notag \\
  &amp;= \begin{pmatrix}
      \sigma_1(u_1) &amp; \cdots &amp; O \\
      \vdots &amp; \ddots &amp; \vdots \\
      O &amp; \cdots &amp; \sigma_n(u_n)
    \end{pmatrix}
    \begin{pmatrix}
      h_1 \\
      \vdots \\
      h_n
    \end{pmatrix}.
    \label{activation}
\end{align}
</p>
<h3>3. \(D_u (W \circ \sigma) (u)(h)\)</h3>
<p>Since
\begin{align*}
  W(u + h) - W(u) = Wh,
\end{align*}
we have
\begin{align*}
  D_h W(u)(h) = Wh.
\end{align*}
Along with (\ref{activation}),
\begin{align*}
  D_h (W \circ \sigma)(u)(h) = W h.
\end{align*}
</p>
</section>
{% endblock %}
