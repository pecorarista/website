{% extends '_layout/_article.njk' %}
{% set lang = 'ja' %}
{% set mathematical = 'true' %}
{% set title = 'Calculating Backpropagation Using Fréchet Derivative' %}
{% set posted = '2022-10-09' %}
{% set updated = '2022-10-14' %}

{% block post %}
<section>
<h2><em>This is a draft.</em></h2>
<p>Consider a multilayer perceptron consisting of \(L\) layers, whose \(\ell\)-th layer has \(n(\ell)\) nodes.
The loss function is the squared error
\begin{align*}
  E(z) =
  \frac{1}{2}
  {
    \left\|
      z  - y
    \right\|
  }^2,
\end{align*}
and the activation function for the \(\ell\)-th layer is
\(\sigma^{(\ell)}\colon \mathbb{R}^{n(\ell)} \to \mathbb{R}^{n(\ell)}\)
and
\(\sigma^{(\ell)}_i\colon \mathbb{R} \to \mathbb{R}\)
The activation function is expressed as
\begin{gather*}
  \sigma^{(\ell)}(x) =
    \begin{pmatrix}
      \sigma_1^{(\ell)}(x_1) \\
      \vdots \\
      \sigma_{n(\ell)}^{(\ell)}\left(x_{n(\ell)}\right)
    \end{pmatrix},\\[5pt]
\end{gather*}
where each \(\sigma^{(\ell)}_i\colon \mathbb{R} \to \mathbb{R}\) is differentiable.
Let \(u^{(\ell)} \in \mathbb{R}^{n(\ell)}\) be the input to the \(\ell\)-th layer
and \(z^{(\ell)} \in \mathbb{R}^{n(\ell)}\) be the output from the \(\ell\)-th layer:
\begin{align*}
  z^{(\ell)} = \sigma^{(\ell)} \left( u^{(\ell)} \right).
\end{align*}
The weight of the \(\ell\)-th layer is
\(W^{(\ell)} \in \mathbb{R}^{n(\ell) \times n(\ell - 1)}\), which is expressed as
\begin{align*}
  W^{(\ell)} =
  \begin{pmatrix}
    w_{1,\,1}^{(\ell)}       &amp; \cdots &amp; w_{1,\,n(\ell - 1)}^{(\ell)} \\
    \vdots                   &amp; \ddots &amp; \vdots \\
    w_{n(\ell),\,1}^{(\ell)} &amp; \cdots &amp; w_{n(\ell),\,n(\ell - 1)}
  \end{pmatrix},
\end{align*}
so this means that
\begin{align*}
  u^{(\ell)} = W^{(\ell)} z^{(\ell - 1)} = W^{(\ell)} \sigma^{(\ell - 1)} \left( u^{(\ell - 1)} \right),\quad
\end{align*}
I use the symbol \(W\) for the linear map \(z^{(\ell - 1)} \mapsto W^{(\ell) } z^{(\ell - 1)}\) as well.
<figure class="centered">
  <figcaption><strong>Figure 1:</strong> Architecture of the neural network.</figcaption>
  <img
    src="/static/images/nn.svg"
    alt="Architecture of the neural network"
    style="background-color: transparent; min-width: 700px; margin-top: 2ex;"
  />
</figure>
</p>
<p>
The goal of this post is to derive the derivative of \(E\) with respect to \(W^{(L)}\)
and \(W^{(L - 1)}\) because other cases are easily derived in similar ways.
First, assume each variable is one-dimensional to aid intuitive understanding.
Then,
\begin{gather*}
  \frac{dE}{dW^{(L)}} =
    \frac{dE}{dz^{(L)}}
    \frac{dz^{(L)}}{du^{(L)}}
    \frac{du^{(L)}}{dW^{(L)}},
\end{gather*}
\begin{gather*}
  \frac{dE}{dW^{(L - 1)}} =
    \frac{dE}{dz^{(L)}}
    \frac{dz^{(L)}}{du^{(L)}}
    \frac{du^{(L)}}{du^{(L - 1)}}
    \frac{du^{(L - 1)}}{dW^{(L - 1)}}.
\end{gather*}
In general cases where dimensions are greater than one, the following analogous formula holds
\begin{align}
  D_{W^{(L)}} E\left(W^{(L)}\right)(H) =
  \left(D_{z^{(L)}} E\left(z^{(L)}\right)
  \circ D_{u^{(L)}} z^{(L)}\left(u^{(L)}\right)
  \circ D_{W^{(L)}} u^{(L)}\left(u^{(L)}\right)
  \right)(H),
  \label{chain-L}
\end{align}
\begin{align}
  &amp;D_{W^{(L - 1)}} E\left(W^{(L - 1)}\right)(H)
  \notag \\
  &amp;= \left(D_{z^{(L)}} E\left(z^{(L)}\right)
  \circ D_{u^{(L)}} z^{(L)}\left(u^{(L)}\right)
  \circ D_{u^{(L - 1)}} u^{(L)}\left(u^{(L - 1)}\right)
  \circ  D_{W^{(L - 1)}} u^{(L - 1)}\left(u^{(L - 1)}\right)
  \right)(H),
  \label{chain-L-1}
\end{align}
where \(D_x f(x_0)(h)\) denotes the Fréchet derivative of \(f\) at a point \(x_0\) with respect to \(x\)
along \(h\).

Calculation of (\ref{chain}) is decomposed into the following calculations:
<ol class="math">
  <li>Calculate \(D_{z} E(z)(h) \) where \(E\colon \mathbb{R}^n \to \mathbb{R}_+\) is defined as \(\displaystyle E = \frac{1}{2}{\left\|z - y\right\|}^2\).</li>
  <li>Calculate \(D_{u} \sigma(u)(h)\) where \(\sigma\colon \mathbb{R}^n \to \mathbb{R}^n\) is of the form \(\displaystyle \sigma(u) = \begin{pmatrix}\sigma_1(u_1) \\ \vdots \\ \sigma_n(u_n)\end{pmatrix}\).</li>
  <li>Calculate \(D_W (W \mapsto Wz) (W) (H) \).</li>
</ol>
\begin{align*}
\end{align*}
</p>
<h4>1. \(D_zE(z)(h)\)</h4>
<p>Note that \({\|U\|}^2 = \langle U,\, U \rangle\) and that \( \langle U,\, V\rangle = \trace (U{}^\top V)\).
\begin{align*}
  E(z + h) - E(z)
  &amp;= \frac{1}{2}\| (z + h) - y \|^2 - \frac{1}{2}\| z - y \|^2 \\
  &amp;= \langle z + h - y ,\, z + h - y \rangle - \frac{1}{2} {\|z - y\|}^2 \\
  &amp;= \frac{1}{2}{\|z - y\|}^2 + \langle h,\, z - y \rangle + \frac{1}{2}{\|h\|}^2 - \frac{1}{2} {\|z - y\|}^2 \\
  &amp;\leq \langle z - y,\, h \rangle + \frac{1}{2} \|h\|^2
\end{align*}
Therefore
\begin{align}
  D_z E(z)(h) = \langle z - y,\, h \rangle.
  \label{derivative-of-norm}
\end{align}
</p>
<h4>2. \(D_u \sigma(u)(h)\)</h4>
<p>Since we assume each \(\sigma_i\) is differentiable,
for any \(\varepsilon &gt; 0\),
we can take \(h_i\) satisfying
\begin{align*}
  | \sigma_i(u_i + h_i) - \sigma_i(u_i) - \sigma_i'(u_i)  h_i  | \leq \varepsilon |h_i|
\end{align*}
for all \(i \in \{1,\,\ldots,\,n\}\). Then,
\begin{align*}
\left\|
  \sigma(u + h) - \sigma(u) -
    \begin{pmatrix}
      \sigma(u_1) h_1 \\
      \vdots \\
      \sigma (u_n) h_n
    \end{pmatrix}
\right\|
&amp;= \sqrt{\sum_{k = 1}^n \left(\sigma_k(u_k + h_k)  - \sigma_k(u_k) - \sigma_k(u_k) h_k\right)^2 } \\
&amp;\leq \sqrt{\sum_{k = 1}^n \varepsilon^2 |h_k|^2 }  \\
&amp;= \varepsilon \|h\|
\end{align*}
Defining
\begin{align*}
  J_u =
    \begin{pmatrix}
      \sigma_1(u_1) &amp; \cdots &amp; O \\
      \vdots &amp; \ddots &amp; \vdots \\
      O &amp; \cdots &amp; \sigma_n(u_n)
    \end{pmatrix},
    \quad
    h =
    \begin{pmatrix}
      h_1 \\
      \vdots \\
      h_n
    \end{pmatrix},
\end{align*}
we obtain
\begin{align}
  D_u \sigma(u)(h)
  = J_u h
  \label{derivative-of-activation}
\end{align}
</p>
<h4>3. \(D_W (W \mapsto Wz) (W) (H) \)</h4>
<p>
Since
\begin{align*}
  (W + H) z - W z = Hz,
\end{align*}
we obtain
\begin{align}
  D_W (W \mapsto Wz) (W) (H) = Hz.
  \label{derivative-of-product-right}
\end{align}
</p>
</section>
<section>
<h3>Backpropagation</h3>
<p>Now we can calculate (\ref{chain-L}) and (\ref{chain-L-1}) by
using (\ref{derivative-of-norm}),
(\ref{derivative-of-activation}),
and (\ref{derivative-of-product-right}).
First, (\ref{chain-L}) is calculated as
\begin{align}
  D_{W^{(L)}} E\left(W^{(L)}\right)(H)
  &amp;=
  \left(D_{z^{(L)}} E\left(z^{(L)}\right)
  \circ D_{u^{(L)}} z^{(L)}\left(u^{(L)}\right)
  \circ D_{W^{(L)}} u^{(L)}\left(u^{(L)}\right)
  \right)(H) \notag \\
  &amp;=
  \left( h \mapsto \left\langle z^{(L)} - y,\, h \right\rangle \right)
  \circ \left( h \mapsto J^{(L)}_{u^{(L)}} \right)
	\circ \left(H \mapsto H z^{(L - 1)}\right)(H) \notag \\
  &amp;=
  \left\langle z^{(L)} - y,\, J^{(L)}_{u^{(L)}} H z^{(L - 1)} \right\rangle \notag \\
  &amp;= h \trace \left( \left(z^{(L)} - y\right)^\top J^{(L)}_{u^{(L)}} E_{ij} z^{(L - 1)} \right) \notag \\
  &amp;= h \trace \left( z^{(L - 1)}{} \left(z^{(L)} - y\right)^\top J^{(L)}_{u^{(L)}} E_{ij} \right) \notag \\
  &amp;= h \trace \left( {\left(J^{(L)}_{u^{(L)}} \left(z^{(L)} - y\right) z^{(L - 1)}{}^\top \right)}^\top E_{ij} \right)
  \label{result-L}
\end{align}
Note that \(J^{(L)}_{u^{(L)}} = J^{(L)}_{u^{(L)}}{}^\top\) beacuse \(J^{(L)}_{u^{(L)}}\) is symmetric.
Each \(\displaystyle \frac{\partial E}{\partial w_{i,\, j}^{(L)}}\) is computed by letting \(H\) be
\begin{gather*}
  \begin{array}{c}
      \begin{array}{ccccccc}
                  &amp;        &amp;   &amp; \text{\(j\)-th} &amp;   &amp;        &amp; \\
                  H = hE_{ij},\quad
      E_{ij} = (0 &amp; \cdots &amp; 0 &amp; he_i &amp; 0   &amp; \cdots &amp; 0),\quad
      \end{array}
  \end{array}
  \begin{array}{c}
      \begin{array}{ccccccc}
               &amp;        &amp;   &amp; \text{\(i\)-th} &amp;   &amp;        &amp; \\
      e_i = (0 &amp; \cdots &amp; 0 &amp; 1 &amp; 0   &amp; \cdots &amp; 0)^\top,
      \end{array}
  \end{array}
\end{gather*}
and \(A =  J^{(L)}_{u^{(L)}} \left(z^{(L)} - y\right) z^{(L - 1)}{}^\top  \). Then,
\begin{align*}
  &amp;\;
  \begin{array}{c}
    \begin{array}{ccccccc}
      &amp;        &amp;   &amp; \text{\(j\)-th} &amp;   &amp;        &amp; \\
      \displaystyle \frac{\partial E}{\partial w_{ij}}
      = \trace (0 &amp; \cdots &amp; 0 &amp; (\text{\(A\)'s \(i\)-th row})^\top &amp; 0   &amp; \cdots &amp; 0)
      = [A]_{i,\,j}
    \end{array}
  \end{array}
\end{align*}
where \([A]_{i,\,j}\) is the \((i,\,j)\)-the element of a matrix \(A\).
</p>
<p>Similarly, (\ref{chain-L-1}) is calculated as
\begin{align*}
  &amp;D_{W^{(L - 1)}} E\left(W^{(L - 1)}\right)(H)
  \\
  &amp;= \left(D_{z^{(L)}} E\left(z^{(L)}\right)
  \circ D_{u^{(L)}} z^{(L)}\left(u^{(L)}\right)
  \circ D_{u^{(L - 1)}} u^{(L)}\left(u^{(L - 1)}\right)
  \circ  D_{W^{(L - 1)}} u^{(L - 1)}\left(u^{(L - 1)}\right)
  \right)(H) \\
  &amp;= \left( h \mapsto \left\langle z^{(L)} - y,\, h \right\rangle \right)
  \circ \left( h \mapsto J^{(L)}_{u^{(L)}} h \right)
  \circ \left( h \mapsto W^{(L)} J^{(L - 1)}_{u^{(L - 1)}} h  \right)
  \circ \left( H \mapsto H z^{(L - 1)} \right)(H) \\
  &amp;= \left\langle z^{(L)} - y,\, J^{(L)}_{u^{(L)}} W^{(L)} J^{(L - 1)}_{u^{(L - 1)}} H z^{(L - 2)} \right\rangle \\
  &amp;= \trace \left( (z^{(L)} - y) J^{(L)}_{u^{(L)}} W^{(L)} J^{(L - 1)}_{u^{(L - 1)}} H z^{(L - 2)} \right).
\end{align*}
</p>
</section>
{% endblock %}
